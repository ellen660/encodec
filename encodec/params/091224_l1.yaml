#batch size, debug, weights, train_discriminator, discriminator start epoch, learning rate

exp_details:
  name: '091224_l1'
  description: '4 hours at 10 hz'
  date: '2024-12-15'

common:
  log_interval: 5
  max_epoch: 600
  seed: 42
  gradient_clipping: True

dataset:
  batch_size: 80 #for no discrim
  #for gpu10, 88 - 24
  #for gpu07, 48 - 8
  max_length: 144000 # 4 hours at 10 hz
  num_workers: 10
  debug: False
  cv: 0
  mgh: 1.
  shhs1: 1.
  shhs2: 1.
  mros1: 1.
  mros2: 1.
  wsc: 1.
  cfs: 1.
  bwh: 1.
  thorax: 0.5
  abdominal: 0.5
  rf: 0.

checkpoint:
  # save: False
  save_every: 50

optimization:
  lr: 2e-4 #base learning rate 1e-4
  # weight_decay: 1e-6
  disc_lr: 3e-4 #base discriminator learning rate 3e-4

loss:
  weight_l1: 1.0
  weight_l2: 0.01
  weight_commit: 0.25
  weight_freq: 0.25
  weight_g: 3.
  weight_feat: 3.
  alpha: 0.01
  bandwidth: null
  n_fft: 1024 # 300 then 512 then 1024
  # hop_length: [20, 50, 256] #try smaller hop length?
  # win_length: [100, 300, 1024]
  #default was 1024, 300, 50 for 5 seconds
  #n_fft = number frequency bins
  #hop_length = next window hop
  #win_length = window size
  commit_start_epoch: 30

lr_scheduler:
  warmup_epoch: 10

model:
  # ratios: [5, 5, 2, 1] # 50 downsampling 5 seconds
  ratios: [6,5,5,2,1] # 300 downsampling 30 seconds
  # ratios: [5,5,4,1] #100 downsampling 10 seconds
  # ratios: [6,5,5,1] # 150 downsampling 15 seconds
  # ratios: [10, 6, 5, 1]# 30 seconds
  bins: 512
  dimension: 256
  target_bandwidths: [0.32] #0.1 is 10 codebooks, 0.2 is 20 codebooks
  train_discriminator: False # you can set it to 2/3 and other number to train discriminator only
  train_discriminator_start_epoch: 60
  train_discriminator_for: 0
  train_discriminator_prob: 0.25
  disc_hop_lengths: [128, 256]
  disc_win_lengths: [512, 1024]
  disc_n_ffts: [1024, 1024]
  filters: 32
  audio_normalize: False
  causal: True
  # norm: 'weight_norm'
  norm: 'layer_norm'
  segment: None
  name: 'my_encodec'
  sample_rate: 10
  channels: 1

distributed:
  data_parallel: True
  world_size: 8 

# balancer:
#   weights:
#     l_t: 1
#     l_f: 1
#     # l_g: 3
#     # l_feat: 3

#i feel like logically,
#if downsamplig larger
#decrease bins
#but increase feature size